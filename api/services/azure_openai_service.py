"""
Azure OpenAI Service for intelligent data analysis and SQL generation
"""

import asyncio
import json
from typing import Dict, List, Any, Optional, Union
import pandas as pd
from openai import AsyncAzureOpenAI
import sqlparse

from core.config import get_azure_settings
from utils.logger import logger
from utils.exceptions import AIProcessingError


class AzureOpenAIService:
    """Service for Azure OpenAI integration"""
    
    def __init__(self):
        self.settings = get_azure_settings()
        self.client = None
        self.is_configured = self._check_configuration()
        if self.is_configured:
            self.client = AsyncAzureOpenAI(
                azure_endpoint=self.settings.azure_openai_endpoint,
                api_key=self.settings.azure_openai_api_key,
                api_version=self.settings.azure_openai_api_version
            )
            logger.info("ðŸ¤– Azure OpenAI Service initialized")
        else:
            logger.warning("Azure OpenAI not configured - using fallback analysis")
    
    def _check_configuration(self) -> bool:
        """Check if Azure OpenAI is properly configured"""
        required_settings = [
            self.settings.azure_openai_endpoint,
            self.settings.azure_openai_api_key,
            self.settings.azure_openai_deployment_name
        ]
        return all(setting.strip() for setting in required_settings)
    
    async def analyze_excel_data(
        self, 
        df: pd.DataFrame, 
        filename: str = "data",
        use_ai: bool = True,
        detected_issues: List[str] = None
    ) -> Dict[str, Any]:
        """
        Analyze Excel data using Azure OpenAI - only called when issues detected
        
        Args:
            df: DataFrame to analyze
            filename: Original filename for context
            use_ai: Whether to use AI analysis
            detected_issues: List of specific issues detected by deterministic analysis
            
        Returns:
            Analysis results with insights and recommendations for fixing issues
        """
        
        if not use_ai or not self.is_configured:
            return self._fallback_analysis(df)
        
        try:
            # Prepare data summary for AI with focus on detected issues
            data_summary = self._prepare_data_summary(df, filename)
            
            # Create AI prompt focused on fixing specific issues
            prompt = self._create_issue_resolution_prompt(data_summary, detected_issues or [])
            
            response = await self._call_azure_openai(prompt)
            
            # Parse AI response
            analysis = self._parse_analysis_response(response)
            
            logger.info(f"âœ… Azure OpenAI issue resolution completed for {filename}")
            return analysis
            
        except Exception as e:
            logger.error(f"ERROR: Azure OpenAI analysis failed: {str(e)}")
            return self._fallback_analysis(df)
    
    async def optimize_json_for_excel(
        self, 
        json_data: Union[List[Dict], Dict], 
        use_ai: bool = True,
        detected_issues: List[str] = None
    ) -> Dict[str, Any]:
        """
        Optimize JSON structure for Excel export - only called when issues detected
        
        Args:
            json_data: JSON data to optimize
            use_ai: Whether to use AI optimization
            detected_issues: List of issues detected in deterministic processing
            
        Returns:
            Optimization recommendations and transformations
        """
        
        if not use_ai or not self.is_configured:
            return self._fallback_json_optimization()
        
        try:
            json_summary = self._prepare_json_summary(json_data)
            
            prompt = self._create_json_excel_issue_resolution_prompt(
                json_summary, detected_issues or []
            )
            
            response = await self._call_azure_openai(prompt)
            result = self._parse_json_optimization_response(response)
            
            logger.info(f"âœ… AI JSON to Excel optimization completed")
            return result
            
        except Exception as e:
            logger.error(f"ERROR: AI JSON optimization failed: {str(e)}")
            return self._fallback_json_optimization()
    
    def _create_json_excel_issue_resolution_prompt(
        self, 
        json_summary: Dict[str, Any],
        detected_issues: List[str]
    ) -> str:
        """Create prompt for resolving JSON to Excel conversion issues"""
        
        issues_description = "\n".join([f"- {issue}" for issue in detected_issues])
        
        return f"""
You are a data conversion expert. The JSON to Excel conversion detected issues:

JSON STRUCTURE:
- Total records: {json_summary['total_records']}
- All keys: {json_summary['all_keys']}
- Nested keys: {json_summary['nested_keys']}
- Structure type: {json_summary['structure_type']}

SAMPLE DATA:
{json_summary['sample_data']}

DETECTED ISSUES:
{issues_description}

Please provide specific recommendations to fix these JSON to Excel conversion issues:
1. How to handle each detected issue
2. Data transformation recommendations
3. Excel compatibility optimizations
4. Performance considerations

Focus ONLY on fixing the detected issues for Excel export. Be practical and concise.

Return your response as a JSON object with:
{{
    "confidence": <number 0-100>,
    "optimization_type": "json_to_excel",
    "optimizations": [list of specific optimization steps],
    "recommendations": [list of conversion recommendations],
    "excel_compatibility": [list of Excel-specific fixes]
}}
"""
    
    def _parse_json_optimization_response(self, response: str) -> Dict[str, Any]:
        """Parse AI response for JSON optimization"""
        try:
            import json
            # Try to extract JSON from response
            start = response.find('{')
            end = response.rfind('}') + 1
            
            if start >= 0 and end > start:
                json_str = response[start:end]
                return json.loads(json_str)
            else:
                # Fallback parsing
                return {
                    "confidence": 75,
                    "optimization_type": "json_to_excel",
                    "optimizations": ["Check AI response format"],
                    "recommendations": [],
                    "excel_compatibility": []
                }
        except Exception as e:
            logger.error(f"Failed to parse JSON optimization AI response: {str(e)}")
            return {
                "confidence": 50,
                "optimization_type": "error",
                "optimizations": [f"Parse error: {str(e)}"],
                "recommendations": [],
                "excel_compatibility": []
            }

    async def optimize_json_for_excel_old(
        self, 
        json_data: Union[List[Dict], Dict], 
        use_ai: bool = True
    ) -> Dict[str, Any]:
        """
        Optimize JSON structure for Excel export using AI
        
        Args:
            json_data: JSON data to optimize
            use_ai: Whether to use AI optimization
            
        Returns:
            Optimization recommendations and transformations
        """
        
        if not use_ai or not self.is_configured:
            return self._fallback_json_optimization()
        
        try:
            # Prepare JSON summary
            json_summary = self._prepare_json_summary(json_data)
            
            prompt = self._create_json_optimization_prompt(json_summary)
            
            response = await self._call_azure_openai(prompt)
            
            # Parse optimization response
            optimization = self._parse_optimization_response(response)
            
            logger.info("âœ… Azure OpenAI JSON optimization completed")
            return optimization
            
        except Exception as e:
            logger.error(f"ERROR: Azure OpenAI optimization failed: {str(e)}")
            return self._fallback_json_optimization()
    
    async def generate_sql_insert(
        self, 
        df: pd.DataFrame, 
        table_name: str, 
        use_ai: bool = True,
        detected_issues: List[str] = None
    ) -> Dict[str, Any]:
        """
        Generate SQL INSERT statements - only called when issues detected
        
        Args:
            df: DataFrame to convert
            table_name: Target table name
            use_ai: Whether to use AI assistance
            detected_issues: List of SQL issues detected in deterministic generation
            
        Returns:
            Improved SQL generation recommendations
        """
        
        if not use_ai or not self.is_configured:
            return self._fallback_sql_generation("INSERT")
        
        try:
            data_sample = self._prepare_sql_data_sample(df)
            
            prompt = self._create_sql_issue_resolution_prompt(
                data_sample, table_name, "INSERT", detected_issues or []
            )
            
            response = await self._call_azure_openai(prompt)
            result = self._parse_sql_response(response)
            
            logger.info(f"âœ… AI SQL INSERT issue resolution completed")
            return result
            
        except Exception as e:
            logger.error(f"ERROR: AI SQL INSERT generation failed: {str(e)}")
            return self._fallback_sql_generation("INSERT")
    
    async def generate_sql_update(
        self, 
        df: pd.DataFrame, 
        table_name: str, 
        key_columns: List[str],
        use_ai: bool = True,
        detected_issues: List[str] = None
    ) -> Dict[str, Any]:
        """
        Generate SQL UPDATE statements - only called when issues detected
        
        Args:
            df: DataFrame to convert
            table_name: Target table name  
            key_columns: Key columns for WHERE clause
            use_ai: Whether to use AI assistance
            detected_issues: List of SQL issues detected in deterministic generation
            
        Returns:
            Improved SQL generation recommendations
        """
        
        if not use_ai or not self.is_configured:
            return self._fallback_sql_generation("UPDATE")
        
        try:
            data_sample = self._prepare_sql_data_sample(df)
            
            prompt = self._create_sql_issue_resolution_prompt(
                data_sample, table_name, "UPDATE", detected_issues or [], key_columns
            )
            
            response = await self._call_azure_openai(prompt)
            result = self._parse_sql_response(response)
            
            logger.info(f"âœ… AI SQL UPDATE issue resolution completed")
            return result
            
        except Exception as e:
            logger.error(f"ERROR: AI SQL UPDATE generation failed: {str(e)}")
            return self._fallback_sql_generation("UPDATE")
    
    def _create_sql_issue_resolution_prompt(
        self, 
        data_sample: Dict[str, Any], 
        table_name: str, 
        sql_type: str,
        detected_issues: List[str],
        key_columns: List[str] = None
    ) -> str:
        """Create prompt for resolving SQL generation issues"""
        
        issues_description = "\n".join([f"- {issue}" for issue in detected_issues])
        key_columns_info = f"\nKey columns for WHERE clause: {key_columns}" if key_columns else ""
        
        return f"""
You are a SQL expert. The deterministic SQL {sql_type} generation encountered issues:

TABLE: {table_name}
SQL TYPE: {sql_type}
COLUMNS: {data_sample['columns']}
DATA TYPES: {data_sample['data_types']}
SAMPLE DATA: {data_sample['sample_rows']}
TOTAL ROWS: {data_sample['total_rows']}{key_columns_info}

DETECTED ISSUES:
{issues_description}

Please provide specific recommendations to fix these SQL generation issues:
1. How to handle each detected issue
2. Best practices for SQL {sql_type} generation
3. Performance optimizations
4. Data type handling recommendations

Focus ONLY on fixing the detected issues. Be practical and concise.

Return your response as a JSON object with:
{{
    "confidence": <number 0-100>,
    "sql_type": "{sql_type}",
    "recommendations": [list of specific SQL fix recommendations],
    "optimizations": [list of performance improvements],
    "data_handling": [list of data type and escaping recommendations]
}}
"""
    
    def _parse_sql_response(self, response: str) -> Dict[str, Any]:
        """Parse AI response for SQL generation"""
        try:
            import json
            # Try to extract JSON from response
            start = response.find('{')
            end = response.rfind('}') + 1
            
            if start >= 0 and end > start:
                json_str = response[start:end]
                return json.loads(json_str)
            else:
                # Fallback parsing
                return {
                    "confidence": 75,
                    "sql_type": "UNKNOWN",
                    "recommendations": ["Check AI response format"],
                    "optimizations": [],
                    "data_handling": []
                }
        except Exception as e:
            logger.error(f"Failed to parse SQL AI response: {str(e)}")
            return {
                "confidence": 50,
                "sql_type": "ERROR",
                "recommendations": [f"Parse error: {str(e)}"],
                "optimizations": [],
                "data_handling": []
            }
    
    def _fallback_sql_generation(self, sql_type: str) -> Dict[str, Any]:
        """Fallback for SQL generation when AI is not available"""
        return {
            "confidence": 60,
            "sql_type": sql_type,
            "recommendations": [
                "AI not available for SQL optimization",
                "Using deterministic generation with basic validation"
            ],
            "optimizations": ["Consider manual review of generated SQL"],
            "data_handling": ["Check data types and escaping manually"]
        }

    async def generate_sql_insert_old(
        self, 
        df: pd.DataFrame, 
        table_name: str = "data_table",
        batch_size: int = 100
    ) -> Dict[str, Any]:
        """
        Generate SQL INSERT statements from DataFrame
        
        Args:
            df: DataFrame to convert to SQL
            table_name: Target table name
            batch_size: Number of records per INSERT batch
            
        Returns:
            SQL generation results with statements and metadata
        """
        
        try:
            if not self.is_configured:
                return self._generate_basic_sql_insert(df, table_name, batch_size)
            
            # Prepare data for AI analysis
            data_sample = self._prepare_sql_data_sample(df)
            
            prompt = self._create_sql_insert_prompt(data_sample, table_name)
            
            response = await self._call_azure_openai(prompt)
            
            # Parse SQL response and generate statements
            sql_result = await self._parse_and_generate_sql_insert(response, df, table_name, batch_size)
            
            logger.info(f"âœ… SQL INSERT generation completed for {len(df)} records")
            return sql_result
            
        except Exception as e:
            logger.error(f"ERROR: SQL INSERT generation failed: {str(e)}")
            return self._generate_basic_sql_insert(df, table_name, batch_size)
    
    async def generate_sql_update(
        self, 
        df: pd.DataFrame, 
        table_name: str = "data_table",
        key_columns: List[str] = None
    ) -> Dict[str, Any]:
        """
        Generate SQL UPDATE statements from DataFrame
        
        Args:
            df: DataFrame to convert to SQL UPDATE
            table_name: Target table name
            key_columns: Columns to use as WHERE clause keys
            
        Returns:
            SQL UPDATE generation results
        """
        
        try:
            if not self.is_configured:
                return self._generate_basic_sql_update(df, table_name, key_columns)
            
            # Auto-detect key columns if not provided
            if not key_columns:
                key_columns = self._detect_key_columns(df)
            
            # Prepare data for AI analysis
            data_sample = self._prepare_sql_data_sample(df)
            
            prompt = self._create_sql_update_prompt(data_sample, table_name, key_columns)
            
            response = await self._call_azure_openai(prompt)
            
            # Parse and generate SQL UPDATE statements
            sql_result = await self._parse_and_generate_sql_update(response, df, table_name, key_columns)
            
            logger.info(f"âœ… SQL UPDATE generation completed for {len(df)} records")
            return sql_result
            
        except Exception as e:
            logger.error(f"ERROR: SQL UPDATE generation failed: {str(e)}")
            return self._generate_basic_sql_update(df, table_name, key_columns)
    
    async def _call_azure_openai(self, prompt: str) -> str:
        """Call Azure OpenAI with the given prompt"""
        
        try:
            response = await asyncio.wait_for(
                self.client.chat.completions.create(
                    model=self.settings.azure_openai_deployment_name,
                    messages=[
                        {
                            "role": "system", 
                            "content": "You are an expert data analyst and SQL developer. Provide detailed, accurate analysis and generate clean, optimized SQL code."
                        },
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=self.settings.ai_max_tokens,
                    temperature=self.settings.ai_temperature
                ),
                timeout=self.settings.ai_timeout
            )
            
            return response.choices[0].message.content
            
        except asyncio.TimeoutError:
            raise AIProcessingError("Azure OpenAI request timed out")
        except Exception as e:
            raise AIProcessingError(f"Azure OpenAI API error: {str(e)}")
    
    def _prepare_data_summary(self, df: pd.DataFrame, filename: str) -> Dict[str, Any]:
        """Prepare data summary for AI analysis"""
        
        # Get basic stats
        numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
        text_cols = df.select_dtypes(include=['object']).columns.tolist()
        datetime_cols = df.select_dtypes(include=['datetime64']).columns.tolist()
        
        # Sample data (first few rows)
        sample_data = df.head(3).to_dict('records')
        
        return {
            "filename": filename,
            "shape": {"rows": len(df), "columns": len(df.columns)},
            "columns": {
                "all": df.columns.tolist(),
                "numeric": numeric_cols,
                "text": text_cols,
                "datetime": datetime_cols
            },
            "sample_data": sample_data,
            "missing_values": df.isnull().sum().to_dict(),
            "data_types": df.dtypes.astype(str).to_dict()
        }
    
    def _prepare_json_summary(self, json_data: Union[List[Dict], Dict]) -> Dict[str, Any]:
        """Prepare JSON summary for optimization analysis"""
        
        if isinstance(json_data, dict):
            data_sample = [json_data]
        else:
            data_sample = json_data[:3] if len(json_data) > 3 else json_data
        
        # Analyze structure
        all_keys = set()
        nested_keys = []
        
        for item in (json_data if isinstance(json_data, list) else [json_data]):
            if isinstance(item, dict):
                all_keys.update(item.keys())
                for key, value in item.items():
                    if isinstance(value, (dict, list)):
                        nested_keys.append(key)
        
        return {
            "total_records": len(json_data) if isinstance(json_data, list) else 1,
            "all_keys": list(all_keys),
            "nested_keys": list(set(nested_keys)),
            "sample_data": data_sample,
            "structure_type": "array" if isinstance(json_data, list) else "object"
        }
    
    def _prepare_sql_data_sample(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Prepare data sample for SQL generation"""
        
        return {
            "columns": df.columns.tolist(),
            "data_types": df.dtypes.astype(str).to_dict(),
            "sample_rows": df.head(3).to_dict('records'),
            "total_rows": len(df),
            "nullable_columns": df.columns[df.isnull().any()].tolist()
        }
    
    def _create_issue_resolution_prompt(self, data_summary: Dict[str, Any], detected_issues: List[str]) -> str:
        """Create prompt focused on resolving specific data issues"""
        
        issues_description = "\n".join([f"- {issue}" for issue in detected_issues])
        
        return f"""
You are a data cleaning expert. A deterministic analysis found issues in this Excel data:

Filename: {data_summary['filename']}
Shape: {data_summary['shape']['rows']} rows Ã— {data_summary['shape']['columns']} columns

DETECTED ISSUES:
{issues_description}

SAMPLE DATA:
{data_summary['sample_data']}

COLUMN TYPES:
{data_summary['data_types']}

MISSING VALUES:
{data_summary['missing_values']}

Please provide:
1. Specific recommendations to fix each detected issue
2. Data cleaning steps needed
3. Confidence level (0-100) for successful processing after fixes
4. Any patterns or insights that explain why these issues occurred

Focus ONLY on practical solutions for the detected issues. Be concise and actionable.

Return your response as a JSON object with:
{{
    "confidence": <number 0-100>,
    "analysis_type": "issue_resolution",
    "detected_patterns": [list of issues being addressed],
    "recommendations": [list of specific fix recommendations],
    "cleaning_steps": [list of data cleaning actions needed]
}}
"""

    def _create_excel_analysis_prompt(self, data_summary: Dict[str, Any]) -> str:
        """Create prompt for Excel data analysis"""
        
        return f"""
Analyze the following Excel data and provide insights:

Filename: {data_summary['filename']}
Shape: {data_summary['shape']['rows']} rows Ã— {data_summary['shape']['columns']} columns

Columns by type:
- Numeric: {data_summary['columns']['numeric']}
- Text: {data_summary['columns']['text']}
- DateTime: {data_summary['columns']['datetime']}

Sample data:
{json.dumps(data_summary['sample_data'], indent=2)}

Missing values per column:
{json.dumps(data_summary['missing_values'], indent=2)}

Please provide:
1. Data quality assessment (score 0-100)
2. Detected patterns and insights
3. Recommendations for data improvement
4. Suggested data transformations
5. Business insights if applicable

Format your response as JSON with keys: confidence, patterns, recommendations, insights, quality_score.
"""
    
    def _create_json_optimization_prompt(self, json_summary: Dict[str, Any]) -> str:
        """Create prompt for JSON optimization"""
        
        return f"""
Optimize the following JSON structure for Excel export:

Structure type: {json_summary['structure_type']}
Total records: {json_summary['total_records']}
All keys: {json_summary['all_keys']}
Nested keys: {json_summary['nested_keys']}

Sample data:
{json.dumps(json_summary['sample_data'], indent=2)}

Please provide:
1. Optimal column naming strategy
2. Data flattening recommendations
3. Type conversion suggestions
4. Excel-specific optimizations
5. Layout recommendations

Format your response as JSON with keys: column_mapping, flattening_strategy, type_conversions, excel_optimizations.
"""
    
    def _create_sql_insert_prompt(self, data_sample: Dict[str, Any], table_name: str) -> str:
        """Create prompt for SQL INSERT generation"""
        
        return f"""
Generate optimized SQL INSERT statements for the following data:

Table name: {table_name}
Columns: {data_sample['columns']}
Data types: {json.dumps(data_sample['data_types'], indent=2)}
Sample data: {json.dumps(data_sample['sample_rows'], indent=2)}
Total rows: {data_sample['total_rows']}

Please provide:
1. CREATE TABLE statement with appropriate data types
2. Optimized INSERT statement template
3. Data type mapping recommendations
4. Performance optimization suggestions

Format your response as JSON with keys: create_table, insert_template, data_mappings, optimizations.
"""
    
    def _create_sql_update_prompt(self, data_sample: Dict[str, Any], table_name: str, key_columns: List[str]) -> str:
        """Create prompt for SQL UPDATE generation"""
        
        return f"""
Generate optimized SQL UPDATE statements for the following data:

Table name: {table_name}
Key columns: {key_columns}
All columns: {data_sample['columns']}
Data types: {json.dumps(data_sample['data_types'], indent=2)}
Sample data: {json.dumps(data_sample['sample_rows'], indent=2)}

Please provide:
1. Optimized UPDATE statement template
2. WHERE clause strategy using key columns
3. Batch update recommendations
4. Performance considerations

Format your response as JSON with keys: update_template, where_strategy, batch_recommendations, performance_tips.
"""
    
    def _parse_analysis_response(self, response: str) -> Dict[str, Any]:
        """Parse AI analysis response"""
        
        try:
            # Try to extract JSON from response
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
        except:
            pass
        
        # Fallback to structured parsing
        return {
            "confidence": 85,
            "patterns": ["AI analysis completed"],
            "recommendations": ["Data structure is suitable for processing"],
            "insights": ["Professional data analysis performed"],
            "quality_score": 85
        }
    
    def _parse_optimization_response(self, response: str) -> Dict[str, Any]:
        """Parse AI optimization response"""
        
        try:
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
        except:
            pass
        
        return {
            "column_mapping": {},
            "flattening_strategy": "basic",
            "type_conversions": {},
            "excel_optimizations": ["Apply basic formatting"]
        }
    
    def _fallback_analysis(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Fallback analysis when AI is not available"""
        
        return {
            "confidence": 75,
            "patterns": ["Basic deterministic analysis"],
            "recommendations": ["Enable Azure OpenAI for advanced insights"],
            "insights": ["Data structure appears valid"],
            "quality_score": 75,
            "ai_enabled": False
        }
    
    def _fallback_json_optimization(self) -> Dict[str, Any]:
        """Fallback JSON optimization"""
        
        return {
            "column_mapping": {},
            "flattening_strategy": "basic",
            "type_conversions": {},
            "excel_optimizations": ["Basic formatting applied"],
            "ai_enabled": False
        }
    
    def _detect_key_columns(self, df: pd.DataFrame) -> List[str]:
        """Auto-detect potential key columns"""
        
        key_candidates = []
        
        for col in df.columns:
            col_lower = col.lower()
            # Look for ID columns
            if 'id' in col_lower or col_lower.endswith('_id'):
                key_candidates.append(col)
            # Look for unique identifiers
            elif df[col].nunique() == len(df) and not df[col].isnull().any():
                key_candidates.append(col)
        
        return key_candidates[:2] if key_candidates else [df.columns[0]]
    
    def _generate_basic_sql_insert(self, df: pd.DataFrame, table_name: str, batch_size: int) -> Dict[str, Any]:
        """Generate basic SQL INSERT without AI"""
        
        # Create basic CREATE TABLE statement
        create_table = f"CREATE TABLE {table_name} (\n"
        for col in df.columns:
            dtype = df[col].dtype
            if 'int' in str(dtype):
                sql_type = "INTEGER"
            elif 'float' in str(dtype):
                sql_type = "DECIMAL(10,2)"
            elif 'datetime' in str(dtype):
                sql_type = "DATETIME"
            else:
                sql_type = "VARCHAR(255)"
            
            create_table += f"    {col} {sql_type},\n"
        
        create_table = create_table.rstrip(',\n') + "\n);"
        
        # Create INSERT template
        columns_str = ", ".join(df.columns)
        placeholders = ", ".join(["?" for _ in df.columns])
        insert_template = f"INSERT INTO {table_name} ({columns_str}) VALUES ({placeholders});"
        
        return {
            "create_table": create_table,
            "insert_template": insert_template,
            "total_records": len(df),
            "batch_size": batch_size,
            "ai_enabled": False
        }
    
    def _generate_basic_sql_update(self, df: pd.DataFrame, table_name: str, key_columns: List[str]) -> Dict[str, Any]:
        """Generate basic SQL UPDATE without AI"""
        
        if not key_columns:
            key_columns = [df.columns[0]]
        
        # Create UPDATE template
        set_columns = [col for col in df.columns if col not in key_columns]
        set_clause = ", ".join([f"{col} = ?" for col in set_columns])
        where_clause = " AND ".join([f"{col} = ?" for col in key_columns])
        
        update_template = f"UPDATE {table_name} SET {set_clause} WHERE {where_clause};"
        
        return {
            "update_template": update_template,
            "key_columns": key_columns,
            "update_columns": set_columns,
            "total_records": len(df),
            "ai_enabled": False
        }
    
    async def _parse_and_generate_sql_insert(self, response: str, df: pd.DataFrame, table_name: str, batch_size: int) -> Dict[str, Any]:
        """Parse AI response and generate SQL INSERT statements"""
        
        try:
            # Parse AI response
            ai_result = self._parse_analysis_response(response)
            
            # Generate statements based on AI recommendations
            create_table = ai_result.get('create_table', '')
            insert_template = ai_result.get('insert_template', '')
            
            # If AI didn't provide templates, fall back to basic generation
            if not create_table or not insert_template:
                return self._generate_basic_sql_insert(df, table_name, batch_size)
            
            return {
                "create_table": create_table,
                "insert_template": insert_template,
                "total_records": len(df),
                "batch_size": batch_size,
                "ai_recommendations": ai_result.get('optimizations', []),
                "ai_enabled": True
            }
            
        except Exception as e:
            logger.warning(f"Failed to parse AI SQL response: {str(e)}")
            return self._generate_basic_sql_insert(df, table_name, batch_size)
    
    async def _parse_and_generate_sql_update(self, response: str, df: pd.DataFrame, table_name: str, key_columns: List[str]) -> Dict[str, Any]:
        """Parse AI response and generate SQL UPDATE statements"""
        
        try:
            # Parse AI response
            ai_result = self._parse_analysis_response(response)
            
            update_template = ai_result.get('update_template', '')
            
            # If AI didn't provide template, fall back to basic generation
            if not update_template:
                return self._generate_basic_sql_update(df, table_name, key_columns)
            
            return {
                "update_template": update_template,
                "key_columns": key_columns,
                "total_records": len(df),
                "ai_recommendations": ai_result.get('performance_tips', []),
                "ai_enabled": True
            }
            
        except Exception as e:
            logger.warning(f"Failed to parse AI SQL response: {str(e)}")
            return self._generate_basic_sql_update(df, table_name, key_columns)
